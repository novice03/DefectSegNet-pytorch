{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation details\n",
    "The purpose of this notebook is to provide details about the implementation of DefectSegNet in defectsegnet.py. \n",
    "\n",
    "The model is described in the paper as follows:\n",
    "\n",
    "\"The DefectSegNet architecture, shown [below], consists of a total of 19 hidden layers. On the encoder side, max pooling is performed after each dense block, enabling the succeeding block to extract higher level, more contextual (and abstract) features from the defect images. For the decoder, to recover the resolution we employed the transposed convolutions, a more sophisticated operator than bilinear interpolation, for up-sampling. There are equal numbers of max pooling layers and transposed convolution layers, so the output probability map has the same spatial resolution as the input image. For the design of skip connections, besides those already introduced in dense blocks, feature maps created during encoding are input to all the decoder layers of the same spatial resolution. This allows the feature maps of a certain spatial resolution to connect cross the encoder-decoder performing in a similar manner to a single dense block. The incorporation of these skip connections both within and across blocks is the primary difference between our DefectSegNet and the U-Net and the fully convolutional DenseNet. Lastly, the final hidden layer is a 3 Ã— 3 convolutional layer with a sigmoid activation function for classification.\"\n",
    "\n",
    "![model arch](arch.png \"DefectSegNet architecture\")\n",
    "\n",
    "It can be noticed that the model architecture (except the last 2 layers) consits of repeating blocks of either two convolution layers and a max pooling layer or two convolution layers and a transposed convolution layer. There are three of each type followed by 2 convolution layers. A block of layers is implemented in the DenseConvBlock class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, transpose = False):\n",
    "        super().__init__()\n",
    "        self.block_in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block_out_channels = 2 * out_channels + in_channels\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels + out_channels, out_channels, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        if transpose:\n",
    "            self.tconv = nn.ConvTranspose2d(self.block_out_channels, int(out_channels / 2), kernel_size = (2, 2), stride = (2, 2))\n",
    "            self.bn3 = nn.BatchNorm2d(self.tconv.out_channels)\n",
    "        else:\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size = (2, 2), stride = (2, 2))\n",
    "            self.bn3 = nn.BatchNorm2d(self.block_out_channels)\n",
    "                    \n",
    "        self.act3 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, concat_channels = None):\n",
    "        if concat_channels is not None:\n",
    "            c1 = self.act1(self.bn1(self.conv1(torch.cat([x, * concat_channels], dim = 1))))\n",
    "            c2 = self.act2(self.bn2(self.conv2(torch.cat([x, * concat_channels, c1], dim = 1))))\n",
    "            t1 = self.act3(self.bn3(self.tconv(torch.cat([x, * concat_channels, c1, c2], dim = 1))))\n",
    "            \n",
    "            return c1, c2, t1\n",
    "            \n",
    "        else:\n",
    "            c1 = self.act1(self.bn1(self.conv1(x)))\n",
    "            c2 = self.act2(self.bn2(self.conv2(torch.cat([x, c1], dim = 1))))\n",
    "            p1 = self.act3(self.bn3(self.pool1(torch.cat([x, c1, c2], dim = 1))))\n",
    "                    \n",
    "            return c1, c2, p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While creating a DenseConvBlock object, if transpose = True, the block will consist of a transposed convolution layer instead of a max pooling layer. In the forward operation, concat_channels array stores inputs that must be used in the skip connections (if any). Each block returns intermediate outputs that are stored for future use.\n",
    "\n",
    "3 blocks with max pooling, 3 blocks with transposed convolution and 2 convolution layers are combined to form the DefectSegNet architecture below. The elements in concat_channels is based on the official implementation of the model. The in_channels of the transposed convolution block is the sum of the number of channels in input from the previous block and elements in the concat_channels array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectSegNet(nn.Module):\n",
    "    def __init__(self, in_channels = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convblock1 = DenseConvBlock(in_channels = in_channels, out_channels = 4)\n",
    "        self.convblock2 = DenseConvBlock(in_channels = self.convblock1.block_out_channels, out_channels = 16)\n",
    "        self.convblock3 = DenseConvBlock(in_channels = self.convblock2.block_out_channels, out_channels = 32)\n",
    "        \n",
    "        self.tconvblock1 = DenseConvBlock(in_channels = self.convblock3.block_out_channels, out_channels = 64, transpose = True)\n",
    "        self.tconvblock2 = DenseConvBlock(in_channels = int(self.convblock2.block_out_channels + \n",
    "                                                            (2 * self.convblock3.out_channels) + (self.tconvblock1.out_channels / 2)), out_channels = 32, transpose = True)\n",
    "        self.tconvblock3 = DenseConvBlock(in_channels = int(self.convblock1.block_out_channels + \n",
    "                                                            (2 * self.convblock2.out_channels) + (self.tconvblock2.out_channels / 2)), out_channels = 16, transpose = True)\n",
    "        \n",
    "        self.conv13 = nn.Conv2d(int(self.convblock1.block_in_channels + \n",
    "                                   (2 * self.convblock1.out_channels) + (self.tconvblock3.out_channels / 2)), 4, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        \n",
    "        self.bn19 = nn.BatchNorm2d(4)\n",
    "        self.act19 = nn.ReLU()\n",
    "        self.conv14 = nn.Conv2d(int(self.conv13.in_channels + self.conv13.out_channels), 1, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.bn20 = nn.BatchNorm2d(1)\n",
    "        self.act20 = nn.Sigmoid() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        c1, c2, p1 = self.convblock1(x)\n",
    "        c3, c4, p2 = self.convblock2(p1)\n",
    "        c5, c6, p3 = self.convblock3(p2)\n",
    "        \n",
    "        c7, c8, t1 = self.tconvblock1(p3, concat_channels = [])\n",
    "        c9, c10, t2 = self.tconvblock2(t1, concat_channels = [p2, c5, c6])\n",
    "        c11, c12, t3 = self.tconvblock3(t2, concat_channels = [p1, c3, c4])\n",
    "        \n",
    "        c13 = self.act19(self.bn19(self.conv13(torch.cat([x, c1, c2, t3], dim = 1))))\n",
    "        c14 = self.act20(self.bn20(self.conv14(torch.cat([x, c1, c2, t3, c13], dim = 1))))\n",
    "        \n",
    "        return c14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefectSegNet(\n",
       "  (convblock1): DenseConvBlock(\n",
       "    (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn3): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "  )\n",
       "  (convblock2): DenseConvBlock(\n",
       "    (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(25, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn3): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "  )\n",
       "  (convblock3): DenseConvBlock(\n",
       "    (conv1): Conv2d(41, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(73, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn3): BatchNorm2d(105, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "  )\n",
       "  (tconvblock1): DenseConvBlock(\n",
       "    (conv1): Conv2d(105, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(169, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (tconv): ConvTranspose2d(233, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "  )\n",
       "  (tconvblock2): DenseConvBlock(\n",
       "    (conv1): Conv2d(137, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(169, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (tconv): ConvTranspose2d(201, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "  )\n",
       "  (tconvblock3): DenseConvBlock(\n",
       "    (conv1): Conv2d(57, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(73, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (tconv): ConvTranspose2d(89, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (bn3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "  )\n",
       "  (conv13): Conv2d(17, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn19): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act19): ReLU()\n",
       "  (conv14): Conv2d(21, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn20): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act20): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defectsegnet = DefectSegNet()\n",
    "defectsegnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
